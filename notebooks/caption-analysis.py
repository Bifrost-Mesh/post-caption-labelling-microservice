import marimo

__generated_with = "0.14.10"
app = marimo.App(width="full", app_title="Caption Analysis")


@app.cell
def _():
    caption = """
    Spent an amazing night in the heart of Copenhagen, with 2 of my lovely colleagues â¤ï¸
    @codingsafari and @johannesholzel.

    We talked primarily about CloudNative Distributed Systems, Data stream processing using Apache
    Flink and building Recommendation Engines.

    And dammnn! Those pastries were real good....ðŸ˜‹

    #copenhagen #tech #distributed-systems #big-data #data-stream-processing
    """

    import re

    def preprocess_caption(caption):
        replacements = [
            # replace newline characters with simple spaces.
            (r'\r\n|\r|\n', ' '),

            # remove mentions and hashtags.
            (r'@[^\s]+', ''),
            (r'#[^\s]+', '')
        ]

        for replacement in replacements:
            old, new = replacement
            caption = re.sub(old, new, caption)

        return caption

    preprocessed_caption = preprocess_caption(caption)
    print(preprocessed_caption)
    return (preprocessed_caption,)


@app.cell
def _():
    import spacy

    # spaCy provides different models / trained pipelines, which can be installed as Python
    # packages.
    # We'll use the 'en_core_web_sm' model : English pipeline optimized for CPU.
    spacy_model = spacy.load("en_core_web_sm")
    return (spacy_model,)


@app.cell
def _(preprocessed_caption, spacy_model):
    # Processing raw text intelligently is difficult: most words are rare, and itâ€™s common for
    # words that look completely different to mean almost the same thing. The same words in a
    # different order can mean something completely different. Even splitting text into useful
    # word-like units can be difficult in many languages.
    #
    # While itâ€™s possible to solve some problems starting from only the raw characters, itâ€™s
    # usually better to use linguistic knowledge to add useful information. Thatâ€™s exactly what
    # spaCy is designed to do:
    # you put in raw text, and get back a Doc object, that comes with a variety of annotations.
    doc = spacy_model(preprocessed_caption)
    return (doc,)


@app.cell
def _(doc):
    # Linguistic annotations are available as Token attributes.
    #
    # NOTE : Like many NLP libraries, spaCy encodes all strings to hash values to reduce memory
    #        usage and improve efficiency. So to get the readable string representation of an
    #        attribute, we need to add an underscore _ to its name.
    for token in doc:
        print((token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.is_stop))
    return


@app.cell
def _(doc):
    def filter_tokens(doc):

        def filter_token(token):
            return (not token.is_alpha or
                    token.is_stop or
                    token.pos_ not in ["ADJ", "NOUN", "PROPN"] or
                    token.dep_ not in ["nsubj", "dobj", "pobj", "compound", "amod", "appos"])

        filtered_tokens = [token for token in doc if not filter_token(token)]
        return filtered_tokens

    filtered_tokens = filter_tokens(doc)

    for filtered_token in filtered_tokens:
        print((filtered_token.lemma_, filtered_token.pos_, filtered_token.tag_, filtered_token.dep_))
    return (filtered_tokens,)


@app.cell
def _(doc):
    # Noun chunks are â€base noun phrasesâ€ â€“ flat phrases that have a noun as their head. You can
    # think of noun chunks as a noun plus the words describing the noun â€“ for example :
    # â€œthe lavish green grassâ€ or â€œthe worldâ€™s largest tech fundâ€. 
    for noun_chunk in doc.noun_chunks:
        print((noun_chunk.text, noun_chunk.root.text, noun_chunk.root.dep_))
    return


@app.cell
def _(doc):
    # spaCy features an extremely fast statistical entity recognition system, that assigns labels
    # to contiguous spans of tokens. The default trained pipelines can identify a variety of named
    # and numeric entities, including companies, locations, organizations and products.
    for entity in doc.ents:
        print((entity.text, entity.label_))
    return


@app.cell
def _(doc):
    def filter_entities(doc):

        def filter_entity(entity):
            return (entity.label_ not in ["ORG", "PRODUCT"])

        filtered_entities = [entity for entity in doc.ents if not filter_entity(entity)]
        return filtered_entities

    filtered_entities = filter_entities(doc)

    for filtered_entity in filtered_entities:
        print((filtered_entity.text, filtered_entity.label_))
    return (filtered_entities,)


@app.cell
def _(filtered_entities, filtered_tokens):
    caption_labels = list(set(
        [filtered_token.text.lower() for filtered_token in filtered_tokens] +
        [filtered_entity.text.lower() for filtered_entity in filtered_entities]))

    print(caption_labels)
    return (caption_labels,)


@app.cell
def _(caption_labels, preprocessed_caption):
    import keybert
    from sklearn.feature_extraction.text import CountVectorizer
    from sentence_transformers import SentenceTransformer

    # Sentence Transformers (a.k.a. SBERT) is the go-to Python module for accessing, using, and
    # training state-of-the-art embedding and reranker models. It can be used to compute embeddings
    # using Sentence Transformer models, to calculate similarity scores using Cross-Encoder
    # (a.k.a. reranker) models, or to generate sparse embeddings using Sparse Encoder models. This
    # unlocks a wide range of applications, including semantic search, semantic textual similarity,
    # and paraphrase mining.
    #
    # A wide selection of over 10,000 pre-trained Sentence Transformers models are available for
    # immediate use on ðŸ¤— Hugging Face.
    sentence_transformer_model = SentenceTransformer("all-MiniLM-L6-v2")

    # KeyBERT supports quite a few embedding models. Having the option to choose embedding models
    # allow you to leverage pre-trained embeddings that suit your use-case.
    keyBERT_model = keybert.KeyBERT(model=sentence_transformer_model)

    def rank_caption_labels(caption_labels: str) -> str:
        # An unexpectly important component of KeyBERT is the CountVectorizer. In KeyBERT, it is
        # used to split up your documents into candidate keywords and keyphrases.
        # Since we use the vectorizer to split up the documents after embedding them, we can parse
        # the document however we want as it does not affect the quality of the document embeddings.
        vectorizer = CountVectorizer(ngram_range=(1, 3),
                                     vocabulary=caption_labels)

        # As a default, KeyBERT simply compares the documents and candidate keywords/keyphrases
        # based on their cosine similarity. However, this might lead to very similar words ending
        # up in the list of most accurate keywords/keyphrases. To make sure they are a bit more
        # diversified we can use Maximal Margin Relevance (MMR).
        keywords = keyBERT_model.extract_keywords(preprocessed_caption.lower(),
                                                  vectorizer=vectorizer,
                                                  top_n=5,
                                                  use_mmr=True,
                                                  diversity=0.5)

        ranked_caption_labels = [keyword[0] for keyword in keywords]
        return ranked_caption_labels

    ranked_caption_labels = rank_caption_labels(caption_labels)
    print(ranked_caption_labels)
    return ranked_caption_labels, sentence_transformer_model


@app.cell
def _(ranked_caption_labels, sentence_transformer_model):
    post_categories = [
        "travel",
        "technology",
        "photography",
        "music",
        "food",
        "football",
        "dance",
        "cricket",
        "yoga",
        "gym",
        "art"
    ]

    from sentence_transformers import util
    from collections import Counter

    def match_ranked_caption_labels_to_post_categories(ranked_caption_labels):
        ranked_caption_label_encodings = sentence_transformer_model.encode(ranked_caption_labels,
                                                                           convert_to_tensor=True)
        post_category_encodings = sentence_transformer_model.encode(post_categories, convert_to_tensor=True)

        cosine_similarities = util.cos_sim(ranked_caption_label_encodings, post_category_encodings)

        post_category_match_counts = Counter()
        for i, ranked_caption_label in enumerate(ranked_caption_labels):
            cosine_similarity = cosine_similarities[i]

            best_matched_post_category_index = cosine_similarity.argmax().item()
            best_matched_post_category = post_categories[best_matched_post_category_index]

            print({
                "caption label": ranked_caption_label,
                "matched category" : best_matched_post_category,
                "similarity score" : cosine_similarity.max().item()
            })

            post_category_match_counts[best_matched_post_category] += 1

        total_match_count = sum(post_category_match_counts.values())
        post_category_match_percentages = {
            post_category:
                round(post_category_match_count * 100 / total_match_count, 2)

            for post_category, post_category_match_count in post_category_match_counts.items()
        }
        return post_category_match_percentages

    match_percentages = match_ranked_caption_labels_to_post_categories(ranked_caption_labels)
    print(match_percentages)
    return


if __name__ == "__main__":
    app.run()
